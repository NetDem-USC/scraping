{
    "collab_server" : "",
    "contents" : "### BAUSTELLEN:\n# parse_html: boilerpipeR bringt R zum Absturz\n# download_urls: noch nicht vektorisiert\n# Filebenennung Ã¼ber digest() adhoc\n# Parser ausbauen: Links in Docs identifizieren, Video-Content, Metadaten des Artikels (headline, autor, text...) etc.\n\n\n## function to parse and classify urls\nclassify_urls <- function(urls) {\n  # load packages\n  require(urltools)\n  require(digest)\n  # parse url\n  urls_df <- url_parse(urls)\n  # store full url\n  urls_df$url_full <- urls\n  # store unique_id\n  urls_df$uid <- sapply(urls_df$url_full, digest)\n  # generate indicators\n  urls_df$isMainpage <- is.na(urls_df$path)\n  urls_df$isSurvey <- str_detect(urls_df$domain, \"survey\") | str_detect(urls_df$path, \"survey\")\n  urls_df$isSurveyPlatform <- str_detect(urls_df$domain, \"samplicio.us|yougov|mturk.com|myview.com|zoompanel.com|mintvine.com|sample-cube.com|springboardamerica.com|quickrewards.net|sassieshop.com|lb.ocucom.com|oneopinion.com\")\n  urls_df$isRewardingPlatform <- str_detect(urls_df$domain, \"swagbucks.com|mypoints.com|neobux.com|clixsense.com|inboxdollars.com|prizegrab.com|crowdtap.com|instagc.com|comparteunclic.com|ncponline.com\")\n  urls_df$isSearchEngine <- urls_df$domain %in% c(\"google.com\", \"bing.com\", \"search.yahoo.com\", \"search.aol.com\", \"duckduckgo.com\")\n  urls_df$isSearch <- str_detect(urls_df$domain, \"search\") | str_detect(urls_df$path, \"search\")\n  urls_df$isMail <- str_detect(urls_df$domain, \"mail|outlook\")\n  urls_df$isGoogle <- str_detect(urls_df$domain, \"google\")\n  urls_df$isFacebook <- str_detect(urls_df$domain, \"facebook\")\n  urls_df$isTwitter <- str_detect(urls_df$domain, \"twitter\")\n  urls_df$isGooglePlus <- str_detect(urls_df$domain, \"plus.google\")\n  urls_df$isYouTube <- str_detect(urls_df$domain, \"youtube\")\n  urls_df$isInstagram <- str_detect(urls_df$domain, \"instagram\")\n  urls_df$isTumblr <- str_detect(urls_df$domain, \"tumblr\")\n  urls_df$isShopping <- str_detect(urls_df$domain, \"amazon.com|ebay.com|amazon.com|paypal.com|walmart.com|pinterest.com|etsy.com\")\n  urls_df$isGaming <- str_detect(urls_df$domain, \"player.pureplay.com|pogo.com|worldwinner.com|gsn.com|football.fantasysports.yahoo.com\")\n  urls_df$isPorn <- str_detect(urls_df$domain, \"xhamster.com|youporn.com|pornhub.com\")\n  urls_df$isWikipedia <-  str_detect(urls_df$domain, \"wikipedia\")\n  urls_df$isResidual <- str_detect(urls_df$domain, \"ab.entertainmentcrave.com|netflix.com|ss.ktrmr.com|pch.com|sp004.pcrint.net|person.ancestry.com|imdb.com|prod70.datacollectionsite.com|spectrum.pch.com|realtor.com|login.live.com|zillow.com\")\n  news <- c(\"huffingtonpost.com\", \"washingtonpost.com\", \"nytimes.com\", \"foxnews.com\", \"dailykos.com\", \"drudgereport.com\", \"breitbart.com\")\n  urls_df$isNews <- urls_df$domain %in% news\n  # return output\n  return(urls_df)\n}\n\n\n## function to download urls as html\ndownload_urls <- function(url_df, url_var, url_uid, filename_type = c(\"digest\", \"url\"), dest_folder) {\n  # packages\n  require(curl)\n  # setup new handle\n  h <- new_handle()\n  # get data\n  dat <- url_df\n  urls <- dat[,url_var]\n  urls_uid <- dat[,url_uid]\n  # create filename\n  if(filename_type == \"url\") {\n  file_ext <- ifelse(str_detect(urls, \"html$|htm$\"), \"\", \".html\")\n  filenames <- paste0(dest_folder, \"/\", basename(urls), file_ext)\n  }else{\n  filenames <- paste0(dest_folder, \"/\", urls_uid, \".html\")\n  }\n  # download\n  curl_download(url, filenames, handle = h)\n}\n\n# use this for faster download?\n#browseURL(\"https://github.com/jeroenooms/curl/blob/master/examples/sitemap.R\")\n\n\n## function to parse downloaded htmls\nparse_html <- function(url, dest_folder) {\n  # require packages\n  require(xml2)\n  require(rvest)\n  require(boilerpipeR)\n  # create filename\n  url_name <- str_replace(url, \"\\\\.[[:alnum:]]+$\", \"\")\n  filename <- paste0(basename(url_name), \".txt\")\n  # parse file\n  url_parsed <- read_html(url)\n  url_text <- html_text(url_parsed) #what is this?\n  try(main_text <- ArticleExtractor(url_text, asText = FALSE), silent = TRUE)\n  # export file\n  writeLines(main_text, paste0(dest_folder, \"/\", filename))\n}\n\n\n\n## test functions ---------------------------\n\n# prepare urls\nurls <- c(\"http://www.spiegel.de/politik/ausland/scott-pruitt-und-donald-trump-wer-ist-der-neue-chef-der-epa-a-1135234.html\",\n          \"http://www.sueddeutsche.de/politik/sicherheitskonferenz-trump-der-unsichtbare-elefant-1.3385139\",\n          \"http://www.focus.de/wissen/weltraum/gefahr-aus-dem-all-droht-ein-schwarzes-loch-die-erde-zu-verschlingen_id_6658237.html\",\n          \"http://www.bild.de/politik/inland/muenchner-sicherheitskonferenz/russland-aussenminister-lawrow-auf-muenchener-sicherheitskonferenz-50493950.bild.html\",\n          \"http://www.zeit.de/politik/deutschland/2017-02/afd-bjoern-hoecke-parteiausschluss-dresden-entschuldigung\",\n          \"https://www.welt.de/wirtschaft/article162190193/Merkel-kontert-Kritik-an-Exportstaerke-mit-kleinem-Scherz.html\",\n          \"http://www.t-online.de/lifestyle/gesundheit/id_80399498/umweltministerium-verbietet-fleisch-und-fisch-fuer-seine-gaeste.html\")\n\nlibrary(readr)\nlibrary(stringr)\nlibrary(magrittr)\nlibrary(urltools)\nlibrary(dplyr)\nurls_long <- read_csv(\"sample_urls.csv\")\nurls_long_domain <- url_parse(urls_long$urls)$domain\ntable(urls_long_domain) %>% sort(decreasing = T) %>% extract(1:50)\n\n\n# classify urls\nurls_df <- classify_urls(urls_long$urls)\n\nurls_df_filtered <- filter(urls_df,\n                           isMainpage == F,\n                           isSurvey == F,\n                           isSurveyPlatform == F,\n                           isRewardingPlatform == F,\n                           isSearchEngine == F,\n                           isSearch == F,\n                           isMail == F,\n                           isGoogle == F,\n                           isFacebook == F,\n                           isTwitter == F,\n                           isGooglePlus == F,\n                           isInstagram == F,\n                           isYouTube == F,\n                           isTumblr == F,\n                           isShopping == F,\n                           isGaming == F,\n                           isPorn == F,\n                           isWikipedia == F,\n                           isResidual == F,\n                           isNews == T\n                           )\n\nurls_df_filtered <- filter(urls_df, isNews == T, isMainpage == F)\n\nurls_df_filtered <- filter(urls_df, str_detect(domain, \"tumblr\")) %>% View\nnrow(urls_df_filtered)\ntable(urls_df_filtered$domain) %>% sort(decreasing = T) %>% extract(1:50)\n\n\n# download urls\ndownload(urls_df_filtered[1:5,], \"url_full\", \"uid\", filname_type = \"digest\", dest_folder = \"../../data/urls\")\n\nh <- new_handle()\nfor(i in 1:10) {\n  filename <- paste0(\"../../data/urls\", \"/\", urls_df_filtered$uid[i], \".html\")\n  curl_download(urls_df_filtered$url_full[i], filename, handle = h)\n}\n\n\nurls_df %>% filter(!is.na(path)) %>% # don't scrape content from top-level page\n            filter()\nsapply(urls_df_filtered$url_full[1:50], download_urls, url_uid = \"uid\", dest_folder = \"../../data/urls\")\n\n\n\n# parse url files, store text\nurl_files <- list.files(\"../../data/urls\")\nsapply(url_files, parse_html, dest_folder = \"../../data/urls_parsed\")\n\n\n\n#load(\"unique_urls.RData\")\nurls100 <- readLines(\"unique_urls_100k.txt\") # , stringsAsFactors = FALSE\n\nhead(urls100)\ntail(urls100)\n\nurls100 <- gsub(\"\\\"\", \"\", urls100)\n\nsetwd(\"text_try3\")\nfor(i in 1:34175) { #length(urls100)  11655 23069 33000 34176\n\n  page <- \"\"\n  url <- urls100[i]\n  try(page <- getURLContent(paste0(\"http://\", url), .opts=curlOptions(followlocation=TRUE, timeout=2, maxredirs=20)), silent = TRUE)\n  try(maintext <- ArticleExtractor(page, asText = TRUE), silent = TRUE)\n\n  writeLines(maintext, paste0(\"url_\", i, \".txt\")) # saving to individual text files\n\n}\n",
    "created" : 1495755733575.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1063680539",
    "id" : "A90FA068",
    "lastKnownWriteTime" : 1495855348,
    "last_content_update" : 1495855348903,
    "path" : "~/Desktop/Projects/scraping/SCRAP/R/scrape_urls.R",
    "project_path" : "R/scrape_urls.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}