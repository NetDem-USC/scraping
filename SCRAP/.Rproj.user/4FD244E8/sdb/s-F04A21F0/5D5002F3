{
    "collab_server" : "",
    "contents" : "#!/usr/local/bin/Rscript\nsetwd(\"/Users/munzerts/Dropbox/Uni/Projects/2017-CSS-Attention/code/scraping_pages\")\n\nscraper_spiegel_headlines <- function(folder) {\n  # load packages\n  require(httr)\n  require(stringr)\n  require(magrittr)\n  # get headlines\n  url <- \"http://www.spiegel.de/schlagzeilen/index.html\"\n  url_out <- GET(url) %>% content(as = \"text\")\n  # write raw html\n  dir.create(folder, showWarnings = FALSE, recursive = TRUE)\n  datetime <- format(as.POSIXct(Sys.time(), tz = Sys.timezone()), usetz = TRUE)  %>% as.character() %>% str_replace_all(\"[ :]\", \"-\")\n  write(url_out, file = paste0(folder, \"/spiegel-\", datetime, \".html\"))\n}\n\nscraper_spiegel_articles <- function(folderInput, folderOutput) {\n  # load packages\n  require(httr)\n  require(rvest)\n  require(stringr)\n  require(magrittr)\n  require(R.utils)\n  # import htmls\n  htmls <- list.files(folderInput, pattern = \"spiegel.+html$\", full.names = TRUE)\n  htmls <- htmls[length(htmls)] # pick only last html\n  htmls_parsed <- lapply(htmls, read_html)\n  urls_parsed <- lapply(htmls_parsed, function(x) { html_nodes(x, css = \".schlagzeilen-content a\") %>% html_attr(\"href\") })\n  # download article htmls\n  dir.create(folderOutput, showWarnings = FALSE, recursive = TRUE)\n  urls_articles <- urls_parsed %>% unlist \n  urls_articles <- str_subset(urls_articles, \"^/\") %>% paste0(\"http://www.spiegel.de\", .) # exclude full URLs (e.g., http://bento.de)\n  sapply(urls_articles, function(x){\n      destfile <- paste0(folderOutput, \"/\", basename(x))\n      if(!file.exists(destfile)) {\n        evalWithTimeout({try(download.file(x, destfile = destfile, method = \"libcurl\"))},\n                        timeout = 10,\n                        onTimeout = \"silent\")\n      }\n  })\n}\n# do to's:\n  # make try call more robust\n  # add check on file size\n  # add report (how many URLs downloaded, which could not be downloaded)\n  # implement concurrent downloads (see https://github.com/jeroen/curl/blob/master/R/multi.R)\n\n  \nparser_spiegel_articles <- function(folderInput) {\n  # load packages\n  require(rvest)\n  require(purrr)\n  require(magrittr)\n  require(lubridate)\n  require(stringr)\n  htmls <- list.files(folderInput, pattern = \".+html$\", full.names = TRUE)\n  html_list <- lapply(htmls, read_html)\n  page_parser <- function(css, attr = \"\", multi = FALSE) {\n    if (multi == TRUE){\n    sapply(html_list, function(x) { html_nodes(x, css = css) %>% html_text()}) %>% lapply(function(x) {str_replace_all(x, \"\\\\r|\\\\n|\\\\t\", \"\") %>% paste(sep=\" \", collapse=\" \")}) %>% unlist()\n    }else if (str_length(attr) == 0) {\n    sapply(html_list, function(x) { html_nodes(x, css = css) %>% html_text()}) %>% map(1) %>% lapply(function(x) ifelse(is_null(x), \"\", x)) %>%   unlist()\n    }else{\n    sapply(html_list, function(x) { html_nodes(x, css = css) %>% html_attr(attr)}) %>% map(1) %>% lapply(function(x) ifelse(is_null(x), \"\", x)) %>%   unlist()\n    }\n  }\n  headline <- page_parser(\".headline\")\n  headline_intro <- page_parser(\".headline-intro\")\n  datetime <- page_parser(\".timeformat\", attr = \"datetime\") %>% ymd_hms(tz = \"Europe/Berlin\")\n  domain <- page_parser(\".current-channel-name\")\n  text_intro <- page_parser(\".article-intro\")\n  text <- page_parser(\"#js-article-column li , p\", multi = TRUE)\n  articles_dat <- data.frame(outlet = \"http://www.spiegel.de\",\n                             file = htmls,\n                             headline, \n                             headline_intro,\n                             datetime,\n                             domain,\n                             text_intro,\n                             text,\n                             stringsAsFactors = FALSE\n                             )\n  \n  articles_dat\n}\n# to do's\n  # add parameters: author, comments, raw text\n  # clean dump of parsed data into database\n\nscraper_spiegel_headlines(folder = \"data/spiegelonline/indices\")\nscraper_spiegel_articles(folderInput <- \"data/spiegelonline/indices\", folderOutput <- \"data/spiegelonline/articles\")\n#articles_df <- parser_spiegel_articles(\"data/spiegelonline/articles\")\n",
    "created" : 1495755736710.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1713920549",
    "id" : "5D5002F3",
    "lastKnownWriteTime" : 1495754356,
    "last_content_update" : 1495754356,
    "path" : "~/Desktop/Projects/scraping/scraper_spiegel_online.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}